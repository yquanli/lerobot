"""
SmolVLA è®­ç»ƒè„šæœ¬ - Piper 7D æœºå™¨äºº

è¿™ä¸ªè„šæœ¬å°è£…äº†è®­ç»ƒé…ç½®ï¼Œé¿å…æ¯æ¬¡éƒ½è¾“å…¥é•¿å‘½ä»¤è¡Œå‚æ•°ã€‚
åªéœ€ä¿®æ”¹ä¸‹é¢çš„é…ç½®å­—å…¸ï¼Œç„¶åè¿è¡Œï¼š
    python myscripts/train/train_smolvla.py

æ”¯æŒçš„æ“ä½œï¼š
    - è®­ç»ƒ: python myscripts/train/train_smolvla.py
    - éªŒè¯é…ç½®: python myscripts/train/train_smolvla.py --validate-only
    - ç”Ÿæˆå‘½ä»¤: python myscripts/train/train_smolvla.py --print-command
    - æ¢å¤è®­ç»ƒ: python myscripts/train/train_smolvla.py --resume <checkpoint_path>
"""

import argparse
import json
import subprocess
import sys
from dataclasses import dataclass, field
from pathlib import Path
from typing import Any

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root / "src"))


# ============================================================================
# è®­ç»ƒé…ç½® - åœ¨è¿™é‡Œä¿®æ”¹æ‚¨çš„è®­ç»ƒå‚æ•°
# ============================================================================

@dataclass
class TrainingConfig:
    """SmolVLA è®­ç»ƒé…ç½®
    
    å‚æ•°å‚è€ƒ:
    - configuration_smolvla.py: SmolVLA ç­–ç•¥çš„é»˜è®¤é…ç½®
    - lerobot/smolvla_base: é¢„è®­ç»ƒæ¨¡å‹çš„é…ç½®
    """
    
    # ========================================
    # 1. ç­–ç•¥é…ç½®
    # ========================================
    policy_path: str = "lerobot/smolvla_base"
    """é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„æˆ– HuggingFace repo ID"""
    
    # --- VLM æƒé‡åŠ è½½ ---
    load_vlm_weights: bool = False
    """æ˜¯å¦åŠ è½½é¢„è®­ç»ƒçš„ VLM æƒé‡
    
    """
    
    # --- è®­ç»ƒç­–ç•¥ ---
    freeze_vision_encoder: bool = True
    """æ˜¯å¦å†»ç»“è§†è§‰ç¼–ç å™¨
    - True: åªè®­ç»ƒ Action Expertï¼ˆå¿«é€Ÿå¾®è°ƒï¼Œæ¨èï¼‰
    - False: ä¹Ÿè®­ç»ƒè§†è§‰ç¼–ç å™¨ï¼ˆéœ€è¦æ›´å¤šæ•°æ®ï¼Œé¿å…è¿‡æ‹Ÿåˆï¼‰
    
    é»˜è®¤å€¼: Trueï¼ˆæ¥è‡ª configuration_smolvla.pyï¼‰
    """
    
    train_expert_only: bool = True
    """æ˜¯å¦åªè®­ç»ƒ Action Expert å±‚
    - True: åªè®­ç»ƒ expert å±‚ï¼ˆå¿«é€Ÿï¼Œæ¨èï¼‰
    - False: è®­ç»ƒæ•´ä¸ªæ¨¡å‹ï¼ˆæ…¢ï¼Œéœ€è¦æ›´å¤šæ•°æ®ï¼‰
    
    é»˜è®¤å€¼: Trueï¼ˆæ¥è‡ª configuration_smolvla.pyï¼‰
    """
    
    train_state_proj: bool = True
    """æ˜¯å¦è®­ç»ƒ state projection å±‚
    
    é»˜è®¤å€¼: Trueï¼ˆæ¥è‡ª configuration_smolvla.pyï¼‰
    """
    
    # --- å›¾åƒé¢„å¤„ç† ---
    resize_imgs_with_padding: tuple[int, int] | None = (512, 512)
    """å›¾åƒ resize å°ºå¯¸ (width, height)
    - (512, 512): æ ‡å‡†å°ºå¯¸ï¼ˆæ¥è‡ª configuration_smolvla.pyï¼‰
    - None: ä½¿ç”¨åŸå§‹åˆ†è¾¨ç‡ï¼ˆä¸æ¨èï¼Œéœ€è¦æ›´å¤šæ˜¾å­˜ï¼‰
    
    æ³¨æ„: SmolVLA ä¼šè‡ªåŠ¨ä¿æŒå®½é«˜æ¯”å¹¶ padding
    """
    
    # --- ALOHA ä¸“ç”¨é…ç½®ï¼ˆPiper ä¸éœ€è¦ï¼‰ ---
    adapt_to_pi_aloha: bool = False
    """æ˜¯å¦é€‚é… Physical Intelligence çš„ ALOHA ç©ºé—´
    - True: ç”¨äº ALOHA æœºå™¨äºº
    - False: ç”¨äºå…¶ä»–æœºå™¨äººï¼ˆå¦‚ Piperï¼‰
    
    é»˜è®¤å€¼: Trueï¼ˆconfiguration_smolvla.pyï¼‰ï¼Œä½† Piper åº”è¯¥è®¾ä¸º False
    """
    
    use_delta_joint_actions_aloha: bool = False
    """æ˜¯å¦ä½¿ç”¨å…³èŠ‚å¢é‡åŠ¨ä½œï¼ˆALOHA ä¸“ç”¨ï¼‰
    
    æ³¨æ„: ç›®å‰æœªåœ¨ LeRobot ä¸­å®ç°ï¼Œä¿æŒ False
    """
    
    # ========================================
    # 2. æ•°æ®é›†é…ç½®
    # ========================================
    dataset_repo_id: str = "Sprinng/piper_transfer_cube_to_bin"
    """æ•°æ®é›† HuggingFace repo ID"""
    
    # ç‰¹å¾åç§°æ˜ å°„ï¼ˆæ•°æ®é›†åç§° â†’ ç­–ç•¥åç§°ï¼‰
    rename_map: dict[str, str] = field(default_factory=lambda: {
        "observation.images.top_rgb": "observation.images.camera1",
        "observation.images.wrist_rgb": "observation.images.camera2",
        "observation.images.side_rgb": "observation.images.camera3",
    })
    """æ•°æ®é›†ç‰¹å¾åç§°åˆ°ç­–ç•¥ç‰¹å¾åç§°çš„æ˜ å°„"""
    
    # ========================================
    # 3. æ¨¡å‹æ¶æ„é…ç½®
    # ========================================
    state_dim: int = 7
    """çŠ¶æ€ç»´åº¦ï¼ˆPiper: 6å…³èŠ‚ + 1å¤¹çˆª = 7ï¼‰"""
    
    action_dim: int = 7
    """åŠ¨ä½œç»´åº¦ï¼ˆPiper: 6å…³èŠ‚ + 1å¤¹çˆª = 7ï¼‰"""
    
    max_state_dim: int = 32
    """æœ€å¤§çŠ¶æ€ç»´åº¦ï¼ˆç”¨äº paddingï¼‰
    
    é»˜è®¤å€¼: 32ï¼ˆæ¥è‡ª configuration_smolvla.pyï¼‰
    """
    
    max_action_dim: int = 32
    """æœ€å¤§åŠ¨ä½œç»´åº¦ï¼ˆç”¨äº paddingï¼‰
    
    é»˜è®¤å€¼: 32ï¼ˆæ¥è‡ª configuration_smolvla.pyï¼‰
    """
    
    # --- æ—¶é—´é…ç½® ---
    n_obs_steps: int = 1
    """è§‚å¯Ÿæ­¥æ•°ï¼ˆå†å²å¸§æ•°ï¼‰
    
    é»˜è®¤å€¼: 1ï¼ˆæ¥è‡ª configuration_smolvla.pyï¼‰
    """
    
    chunk_size: int = 50
    """é¢„æµ‹çš„åŠ¨ä½œåºåˆ—é•¿åº¦ï¼ˆaction chunkï¼‰
    
    é»˜è®¤å€¼: 50ï¼ˆæ¥è‡ª configuration_smolvla.pyï¼‰
    """
    
    n_action_steps: int = 50
    """æ‰§è¡Œçš„åŠ¨ä½œæ­¥æ•°ï¼ˆæ¯æ¬¡æ¨ç†æ‰§è¡Œå¤šå°‘æ­¥ï¼‰
    
    é»˜è®¤å€¼: 50ï¼ˆæ¥è‡ª configuration_smolvla.pyï¼‰
    æ³¨æ„: n_action_steps <= chunk_size
    """
    
    # ========================================
    # 4. è®­ç»ƒè¶…å‚æ•°
    # ========================================
    batch_size: int = 32
    """è®­ç»ƒæ‰¹æ¬¡å¤§å°
    
    æ¨èå€¼:
    - 32: é€‚åˆ 24GB GPUï¼ˆfreeze_vision_encoder=Trueï¼‰
    - 16: é€‚åˆ 16GB GPU æˆ– freeze_vision_encoder=False
    - 8: æ˜¾å­˜ä¸è¶³æ—¶
    """
    
    training_steps: int = 50000
    """è®­ç»ƒæ€»æ­¥æ•°
    
    æ¨èå€¼:
    - 30000-50000: å¿«é€Ÿå¾®è°ƒï¼ˆfreeze_vision_encoder=Trueï¼‰
    - 100000-200000: æ·±åº¦å¾®è°ƒï¼ˆfreeze_vision_encoder=Falseï¼‰
    - 500000+: ä»å¤´è®­ç»ƒï¼ˆload_vlm_weights=Falseï¼‰
    """
    
    # --- ä¼˜åŒ–å™¨é…ç½® ---
    learning_rate: float | None = None
    """å­¦ä¹ ç‡
    - None: ä½¿ç”¨ç­–ç•¥é»˜è®¤å€¼ï¼ˆ1e-4ï¼Œæ¥è‡ª configuration_smolvla.pyï¼‰
    - è‡ªå®šä¹‰å€¼: ä¾‹å¦‚ 5e-5ï¼ˆç”¨äºæ·±åº¦å¾®è°ƒï¼‰
    """
    
    optimizer_betas: tuple[float, float] | None = None
    """AdamW ä¼˜åŒ–å™¨çš„ beta å‚æ•°
    
    é»˜è®¤å€¼: (0.9, 0.95)ï¼ˆæ¥è‡ª configuration_smolvla.pyï¼‰
    """
    
    optimizer_eps: float | None = None
    """AdamW ä¼˜åŒ–å™¨çš„ epsilon å‚æ•°
    
    é»˜è®¤å€¼: 1e-8ï¼ˆæ¥è‡ª configuration_smolvla.pyï¼‰
    """
    
    optimizer_weight_decay: float | None = None
    """æƒé‡è¡°å‡
    
    é»˜è®¤å€¼: 1e-10ï¼ˆæ¥è‡ª configuration_smolvla.pyï¼‰
    """
    
    grad_clip_norm: float | None = None
    """æ¢¯åº¦è£å‰ªèŒƒæ•°
    
    é»˜è®¤å€¼: 10ï¼ˆæ¥è‡ª configuration_smolvla.pyï¼‰
    """
    
    # --- å­¦ä¹ ç‡è°ƒåº¦å™¨é…ç½® ---
    scheduler_warmup_steps: int | None = None
    """å­¦ä¹ ç‡é¢„çƒ­æ­¥æ•°
    
    é»˜è®¤å€¼: 1000ï¼ˆæ¥è‡ª configuration_smolvla.pyï¼‰
    """
    
    scheduler_decay_steps: int | None = None
    """å­¦ä¹ ç‡è¡°å‡æ­¥æ•°
    
    é»˜è®¤å€¼: 30000ï¼ˆæ¥è‡ª configuration_smolvla.pyï¼‰
    """
    
    scheduler_decay_lr: float | None = None
    """å­¦ä¹ ç‡è¡°å‡åˆ°çš„æœ€ç»ˆå€¼
    
    é»˜è®¤å€¼: 2.5e-6ï¼ˆæ¥è‡ª configuration_smolvla.pyï¼‰
    """
    
    # --- è¯„ä¼°å’Œä¿å­˜ ---
    eval_freq: int = 10000
    """è¯„ä¼°é¢‘ç‡ï¼ˆæ¯å¤šå°‘æ­¥è¯„ä¼°ä¸€æ¬¡ï¼‰"""
    
    save_freq: int = 10000
    """ä¿å­˜é¢‘ç‡ï¼ˆæ¯å¤šå°‘æ­¥ä¿å­˜ä¸€æ¬¡ checkpointï¼‰"""
    
    # ========================================
    # 5. è¾“å‡ºé…ç½®
    # ========================================
    output_dir: str = "outputs/train/piper_smolvla_finetune"
    """è®­ç»ƒè¾“å‡ºç›®å½•"""
    
    job_name: str = "smolvla_transfer_cube_to_bin"
    """ä»»åŠ¡åç§°ï¼ˆç”¨äºæ—¥å¿—å’Œ wandbï¼‰"""
    
    # ========================================
    # 6. æ—¥å¿—é…ç½®
    # ========================================
    use_wandb: bool = True
    """æ˜¯å¦ä½¿ç”¨ Weights & Biases è®°å½•è®­ç»ƒ"""
    
    wandb_project: str | None = "lerobot"
    """W&B é¡¹ç›®åç§°ï¼ˆå¦‚æœ use_wandb=Trueï¼‰"""
    
    wandb_entity: str | None = None
    """W&B å®ä½“ï¼ˆå›¢é˜Ÿæˆ–ç”¨æˆ·åï¼‰"""
    
    wandb_disable_artifact: bool = True  # â† æ–°å¢å‚æ•°
    """æ˜¯å¦ç¦ç”¨ W&B Artifact åŠŸèƒ½
    - True: ç¦ç”¨ Artifactï¼ˆæ¨èï¼Œå‡å°‘å­˜å‚¨å’Œä¸Šä¼ å¼€é”€ï¼‰
    - False: å¯ç”¨ Artifactï¼ˆä¼šè‡ªåŠ¨ä¿å­˜æ¨¡å‹å’Œæ•°æ®é›†ç‰ˆæœ¬ï¼‰
    
    é»˜è®¤å€¼: Trueï¼ˆæ¨èï¼‰
    """

    log_freq: int = 100
    """æ—¥å¿—è®°å½•é¢‘ç‡"""
    
    # ========================================
    # 7. å…¶ä»–é…ç½®
    # ========================================
    device: str = "cuda"
    """è®­ç»ƒè®¾å¤‡ï¼ˆcuda, cpu, mpsï¼‰"""
    
    num_workers: int = 4
    """æ•°æ®åŠ è½½å™¨çš„å·¥ä½œè¿›ç¨‹æ•°"""
    
    seed: int = 1000
    """éšæœºç§å­"""
    
    resume_from_checkpoint: str | None = None
    """ä» checkpoint æ¢å¤è®­ç»ƒçš„è·¯å¾„"""
    
    push_to_hub: bool = False
    """è®­ç»ƒå®Œæˆåæ˜¯å¦æ¨é€åˆ° HuggingFace Hub"""
    
    hub_repo_id: str | None = None
    """HuggingFace Hub ä»“åº“ IDï¼ˆå¦‚æœ push_to_hub=Trueï¼‰"""
    
    # ========================================
    # 8. é«˜çº§é…ç½®ï¼ˆé€šå¸¸ä¸éœ€è¦ä¿®æ”¹ï¼‰
    # ========================================
    vlm_model_name: str | None = None
    """VLM æ¨¡å‹åç§°
    
    é»˜è®¤å€¼: "HuggingFaceTB/SmolVLM2-500M-Video-Instruct"
    ï¼ˆæ¥è‡ª configuration_smolvla.pyï¼‰
    
    æ³¨æ„: ä½¿ç”¨ from_pretrained æ—¶ä¼šè‡ªåŠ¨åŠ è½½ï¼Œé€šå¸¸ä¸éœ€è¦æ‰‹åŠ¨æŒ‡å®š
    """
    
    tokenizer_max_length: int | None = None
    """Tokenizer æœ€å¤§é•¿åº¦
    
    é»˜è®¤å€¼: 48ï¼ˆæ¥è‡ª configuration_smolvla.pyï¼‰
    """
    
    num_steps: int | None = None
    """è§£ç æ­¥æ•°
    
    é»˜è®¤å€¼: 10ï¼ˆæ¥è‡ª configuration_smolvla.pyï¼‰
    """


# ============================================================================
# è®­ç»ƒè„šæœ¬é€»è¾‘
# ============================================================================

def config_to_cli_args(config: TrainingConfig) -> list[str]:
    """å°†é…ç½®å¯¹è±¡è½¬æ¢ä¸ºå‘½ä»¤è¡Œå‚æ•°åˆ—è¡¨"""
    args = ["lerobot-train"]
    
    # ç­–ç•¥åŸºæœ¬é…ç½®
    args.extend([
        f"--policy.path={config.policy_path}",
    ])
    
    # VLM æƒé‡å’Œè®­ç»ƒç­–ç•¥
    args.extend([
        f"--policy.load_vlm_weights={str(config.load_vlm_weights).lower()}",
        f"--policy.freeze_vision_encoder={str(config.freeze_vision_encoder).lower()}",
        f"--policy.train_expert_only={str(config.train_expert_only).lower()}",
        f"--policy.train_state_proj={str(config.train_state_proj).lower()}",
    ])
    
    # ALOHA é€‚é…ï¼ˆPiper åº”è¯¥ä¸º Falseï¼‰
    if not config.adapt_to_pi_aloha:
        args.append(f"--policy.adapt_to_pi_aloha=false")
    
    # å›¾åƒé¢„å¤„ç†
    if config.resize_imgs_with_padding:
        w, h = config.resize_imgs_with_padding
        args.append(f"--policy.resize_imgs_with_padding=[{w},{h}]")
    
    # â­â­â­ ä¿®æ­£ï¼šä½¿ç”¨å®Œæ•´çš„ JSON å¯¹è±¡æ¥æŒ‡å®šç‰¹å¾ç»´åº¦ â­â­â­
    # æ„å»º input_features å¯¹è±¡
    input_features = {
        "observation.state": {
            "type": "STATE",
            "shape": [config.state_dim]
        }
    }
    # è½¬æ¢ä¸º JSON å­—ç¬¦ä¸²å¹¶è½¬ä¹‰å¼•å·
    input_features_json = json.dumps(input_features).replace('"', '\\"')
    args.append(f'--policy.input_features="{input_features_json}"')
    
    # æ„å»º output_features å¯¹è±¡
    output_features = {
        "action": {
            "type": "ACTION",
            "shape": [config.action_dim]
        }
    }
    # è½¬æ¢ä¸º JSON å­—ç¬¦ä¸²å¹¶è½¬ä¹‰å¼•å·
    output_features_json = json.dumps(output_features).replace('"', '\\"')
    args.append(f'--policy.output_features="{output_features_json}"')
    
    # æ¨¡å‹æ¶æ„é…ç½®ï¼ˆå¦‚æœä¸é»˜è®¤å€¼ä¸åŒï¼‰
    if config.max_state_dim != 32:
        args.append(f"--policy.max_state_dim={config.max_state_dim}")
    if config.max_action_dim != 32:
        args.append(f"--policy.max_action_dim={config.max_action_dim}")
    
    # æ—¶é—´é…ç½®ï¼ˆå¦‚æœä¸é»˜è®¤å€¼ä¸åŒï¼‰
    if config.n_obs_steps != 1:
        args.append(f"--policy.n_obs_steps={config.n_obs_steps}")
    if config.chunk_size != 50:
        args.append(f"--policy.chunk_size={config.chunk_size}")
    if config.n_action_steps != 50:
        args.append(f"--policy.n_action_steps={config.n_action_steps}")
    
    # æ•°æ®é›†é…ç½®
    args.append(f"--dataset.repo_id={config.dataset_repo_id}")
    
    if config.rename_map:
        # â­ ä½¿ç”¨å•å¼•å·åŒ…è£¹ JSONï¼Œå†…éƒ¨ä½¿ç”¨åŒå¼•å·
        rename_str = json.dumps(config.rename_map)
        args.append(f"--rename_map='{rename_str}'")
    
    # è®­ç»ƒè¶…å‚æ•°
    args.extend([
        f"--batch_size={config.batch_size}",
        f"--steps={config.training_steps}",
    ])
    
    # ä¼˜åŒ–å™¨é…ç½®ï¼ˆåªè¦†ç›–éé»˜è®¤å€¼ï¼‰
    if config.learning_rate is not None:
        args.append(f"--policy.optimizer_lr={config.learning_rate}")
    if config.optimizer_betas is not None:
        args.append(f"--policy.optimizer_betas={list(config.optimizer_betas)}")
    if config.optimizer_eps is not None:
        args.append(f"--policy.optimizer_eps={config.optimizer_eps}")
    if config.optimizer_weight_decay is not None:
        args.append(f"--policy.optimizer_weight_decay={config.optimizer_weight_decay}")
    if config.grad_clip_norm is not None:
        args.append(f"--policy.optimizer_grad_clip_norm={config.grad_clip_norm}")
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨é…ç½®
    if config.scheduler_warmup_steps is not None:
        args.append(f"--policy.scheduler_warmup_steps={config.scheduler_warmup_steps}")
    if config.scheduler_decay_steps is not None:
        args.append(f"--policy.scheduler_decay_steps={config.scheduler_decay_steps}")
    if config.scheduler_decay_lr is not None:
        args.append(f"--policy.scheduler_decay_lr={config.scheduler_decay_lr}")
    
    # è¯„ä¼°å’Œä¿å­˜
    args.extend([
        f"--eval_freq={config.eval_freq}",
        f"--save_freq={config.save_freq}",
    ])
    
    # è¾“å‡ºé…ç½®
    args.extend([
        f"--output_dir={config.output_dir}",
        f"--job_name={config.job_name}",
    ])
    
    # æ—¥å¿—é…ç½®
    args.append(f"--wandb.enable={str(config.use_wandb).lower()}")
    
    if config.use_wandb:
        if config.wandb_project:
            args.append(f"--wandb.project={config.wandb_project}")
        if config.wandb_entity:
            args.append(f"--wandb.entity={config.wandb_entity}")
        
        # ç¦ç”¨ W&B Artifact
        if config.wandb_disable_artifact:
            args.append(f"--wandb.disable_artifact={str(config.wandb_disable_artifact).lower()}")
    
    args.append(f"--log_freq={config.log_freq}")
    
    # å…¶ä»–é…ç½®
    args.extend([
        f"--policy.device={config.device}",
        f"--num_workers={config.num_workers}",
        f"--seed={config.seed}",
        f"--policy.push_to_hub={str(config.push_to_hub).lower()}",
    ])
    
    if config.push_to_hub and config.hub_repo_id:
        args.append(f"--policy.repo_id={config.hub_repo_id}")
    
    if config.resume_from_checkpoint:
        args.append(f"--resume_from_checkpoint={config.resume_from_checkpoint}")
    
    # é«˜çº§é…ç½®
    if config.vlm_model_name:
        args.append(f"--policy.vlm_model_name={config.vlm_model_name}")
    if config.tokenizer_max_length is not None:
        args.append(f"--policy.tokenizer_max_length={config.tokenizer_max_length}")
    if config.num_steps is not None:
        args.append(f"--policy.num_steps={config.num_steps}")
    
    return args


def validate_config(config: TrainingConfig) -> bool:
    """éªŒè¯é…ç½®çš„æœ‰æ•ˆæ€§"""
    print("=" * 80)
    print("ğŸ” Validating Configuration")
    print("=" * 80)
    
    errors = []
    warnings = []
    
    # æ£€æŸ¥ç»´åº¦
    if config.state_dim <= 0 or config.action_dim <= 0:
        errors.append(f"Invalid dimensions: state_dim={config.state_dim}, action_dim={config.action_dim}")
    
    if config.state_dim > config.max_state_dim:
        errors.append(f"state_dim ({config.state_dim}) > max_state_dim ({config.max_state_dim})")
    
    if config.action_dim > config.max_action_dim:
        errors.append(f"action_dim ({config.action_dim}) > max_action_dim ({config.max_action_dim})")
    
    # æ£€æŸ¥æ—¶é—´é…ç½®
    if config.n_action_steps > config.chunk_size:
        errors.append(f"n_action_steps ({config.n_action_steps}) > chunk_size ({config.chunk_size})")
    
    # æ£€æŸ¥ ALOHA é…ç½®
    if config.adapt_to_pi_aloha and "piper" in config.dataset_repo_id.lower():
        warnings.append(
            "adapt_to_pi_aloha=True but using Piper dataset. "
            "This setting is for Physical Intelligence ALOHA robots. "
            "Consider setting it to False for Piper."
        )
    
    if config.use_delta_joint_actions_aloha:
        errors.append("use_delta_joint_actions_aloha is not implemented yet in LeRobot")
    
    # æ£€æŸ¥è®­ç»ƒç­–ç•¥
    if not config.load_vlm_weights:
        warnings.append(
            "load_vlm_weights=False: Training from scratch. "
            "This requires large amounts of data and training time. "
            "Consider setting it to True for finetuning."
        )
    
    if config.load_vlm_weights and not config.freeze_vision_encoder and config.batch_size > 16:
        warnings.append(
            f"Training vision encoder with batch_size={config.batch_size} may require too much GPU memory. "
            "Consider reducing batch_size to 8-16."
        )
    
    # æ£€æŸ¥è¾“å‡ºç›®å½•
    output_path = Path(config.output_dir)
    if output_path.exists() and any(output_path.iterdir()):
        warnings.append(f"Output directory already exists and is not empty: {output_path}")
    
    # æ£€æŸ¥ resume checkpoint
    if config.resume_from_checkpoint:
        checkpoint_path = Path(config.resume_from_checkpoint)
        if not checkpoint_path.exists():
            errors.append(f"Resume checkpoint not found: {checkpoint_path}")
    
    # æ£€æŸ¥ Hub é…ç½®
    if config.push_to_hub and not config.hub_repo_id:
        errors.append("push_to_hub=True but hub_repo_id is not set")
    
    # æ£€æŸ¥ W&B é…ç½®
    if config.use_wandb and not config.wandb_project:
        warnings.append("wandb.enable=True but wandb_project is not set (will use default)")
    
    
    # æ‰“å°ç»“æœ
    if errors:
        print("\nâŒ Validation FAILED:")
        for err in errors:
            print(f"  â€¢ {err}")
        print()
        return False
    
    if warnings:
        print("\nâš ï¸  Warnings:")
        for warn in warnings:
            print(f"  â€¢ {warn}")
    
    print("\nâœ… Configuration validation passed!")
    print("=" * 80)
    return True


def print_config_summary(config: TrainingConfig):
    """æ‰“å°é…ç½®æ‘˜è¦"""
    print("\n" + "=" * 80)
    print("ğŸ“‹ Training Configuration Summary")
    print("=" * 80)
    
    print("\nğŸ¤– Policy:")
    print(f"  Path:                  {config.policy_path}")
    print(f"  VLM Model:             {config.vlm_model_name or 'default'}")
    print(f"  Load VLM Weights:      {'âœ…' if config.load_vlm_weights else 'âŒ'} {config.load_vlm_weights}")
    print(f"  Freeze Vision:         {'âœ…' if config.freeze_vision_encoder else 'âŒ'} {config.freeze_vision_encoder}")
    print(f"  Train Expert Only:     {'âœ…' if config.train_expert_only else 'âŒ'} {config.train_expert_only}")
    print(f"  Train State Proj:      {'âœ…' if config.train_state_proj else 'âŒ'} {config.train_state_proj}")
    print(f"  Image Resize:          {config.resize_imgs_with_padding}")
    print(f"  Adapt to Pi ALOHA:     {'âœ…' if config.adapt_to_pi_aloha else 'âŒ'} {config.adapt_to_pi_aloha}")
    
    print("\nğŸ“Š Dataset:")
    print(f"  Repo ID:               {config.dataset_repo_id}")
    print(f"  Rename Map:            {len(config.rename_map)} mappings")
    
    print("\nğŸ”¢ Dimensions:")
    print(f"  State Dimension:       {config.state_dim} (max: {config.max_state_dim})")
    print(f"  Action Dimension:      {config.action_dim} (max: {config.max_action_dim})")
    
    print("\nâ±ï¸  Temporal:")
    print(f"  Observation Steps:     {config.n_obs_steps}")
    print(f"  Chunk Size:            {config.chunk_size}")
    print(f"  Action Steps:          {config.n_action_steps}")
    
    print("\nâš™ï¸  Training:")
    print(f"  Batch Size:            {config.batch_size}")
    print(f"  Training Steps:        {config.training_steps:,}")
    print(f"  Learning Rate:         {config.learning_rate or 'default (1e-4)'}")
    print(f"  Warmup Steps:          {config.scheduler_warmup_steps or 'default (1000)'}")
    print(f"  Decay Steps:           {config.scheduler_decay_steps or 'default (30000)'}")
    print(f"  Device:                {config.device}")
    
    print("\nğŸ’¾ Checkpointing:")
    print(f"  Eval Frequency:        every {config.eval_freq} steps")
    print(f"  Save Frequency:        every {config.save_freq} steps")
    
    print("\nğŸ“‚ Output:")
    print(f"  Output Directory:      {config.output_dir}")
    print(f"  Job Name:              {config.job_name}")
    
    print("\nğŸ“ˆ Logging:")
    print(f"  Use W&B:               {'âœ…' if config.use_wandb else 'âŒ'} {config.use_wandb}")
    if config.use_wandb:
        print(f"  W&B Project:           {config.wandb_project or 'default'}")
    print(f"  Log Frequency:         every {config.log_freq} steps")
    
    if config.push_to_hub:
        print(f"\nğŸ¤— HuggingFace Hub:")
        print(f"  Push to Hub:           âœ… True")
        print(f"  Hub Repo ID:           {config.hub_repo_id}")
    
    print("\n" + "=" * 80)


def run_training(config: TrainingConfig, dry_run: bool = False):
    """è¿è¡Œè®­ç»ƒ"""
    
    # æ‰“å°é…ç½®æ‘˜è¦
    print_config_summary(config)
    
    # éªŒè¯é…ç½®
    if not validate_config(config):
        print("\nâŒ Please fix the configuration errors above.")
        sys.exit(1)
    
    # ç”Ÿæˆå‘½ä»¤
    cmd_args = config_to_cli_args(config)
    
    # æ‰“å°å‘½ä»¤
    print("\n" + "=" * 80)
    print("ğŸš€ Training Command")
    print("=" * 80)
    print("\n" + " \\\n  ".join(cmd_args))
    print("\n" + "=" * 80)
    
    if dry_run:
        print("\nâœ… Dry run completed. Command printed above.")
        return
    
    # ç¡®è®¤å¼€å§‹è®­ç»ƒ
    print("\nâ³ Starting training in 3 seconds... (Ctrl+C to cancel)")
    import time
    try:
        time.sleep(3)
    except KeyboardInterrupt:
        print("\nâŒ Training cancelled by user.")
        sys.exit(0)
    
    # è¿è¡Œè®­ç»ƒ
    print("\n" + "=" * 80)
    print("ğŸƒ Running Training...")
    print("=" * 80 + "\n")
    
    try:
        # â­ ä½¿ç”¨ shell=True æ¥è¿è¡Œ CLI å‘½ä»¤
        cmd_str = " ".join(cmd_args)
        result = subprocess.run(cmd_str, shell=True, cwd=project_root)
        sys.exit(result.returncode)
    except KeyboardInterrupt:
        print("\n\nâš ï¸  Training interrupted by user.")
        sys.exit(130)


def main():
    parser = argparse.ArgumentParser(
        description="SmolVLA Training Script for Piper Robot",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # ç›´æ¥å¼€å§‹è®­ç»ƒ
  python myscripts/train/train_smolvla.py
  
  # åªéªŒè¯é…ç½®ï¼ˆä¸è¿è¡Œè®­ç»ƒï¼‰
  python myscripts/train/train_smolvla.py --validate-only
  
  # æ‰“å°å‘½ä»¤ä½†ä¸è¿è¡Œ
  python myscripts/train/train_smolvla.py --print-command
  
  # ä» checkpoint æ¢å¤è®­ç»ƒ
  python myscripts/train/train_smolvla.py --resume outputs/train/piper/checkpoints/050000
        """
    )
    
    parser.add_argument(
        "--validate-only",
        action="store_true",
        help="åªéªŒè¯é…ç½®ï¼Œä¸è¿è¡Œè®­ç»ƒ"
    )
    parser.add_argument(
        "--print-command",
        action="store_true",
        help="æ‰“å°è®­ç»ƒå‘½ä»¤ä½†ä¸æ‰§è¡Œ"
    )
    parser.add_argument(
        "--resume",
        type=str,
        default=None,
        help="ä»æŒ‡å®š checkpoint æ¢å¤è®­ç»ƒ"
    )
    
    args = parser.parse_args()
    
    # åˆ›å»ºé…ç½®
    config = TrainingConfig()
    
    # å¦‚æœæŒ‡å®šäº† resumeï¼Œè¦†ç›–é…ç½®
    if args.resume:
        config.resume_from_checkpoint = args.resume
    
    # æ ¹æ®æ¨¡å¼è¿è¡Œ
    if args.validate_only:
        print_config_summary(config)
        if validate_config(config):
            print("\nâœ… Configuration is valid!")
        else:
            print("\nâŒ Configuration has errors!")
            sys.exit(1)
    elif args.print_command:
        run_training(config, dry_run=True)
    else:
        run_training(config, dry_run=False)


if __name__ == "__main__":
    main()