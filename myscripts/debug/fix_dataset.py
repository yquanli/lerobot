"""
ä¿®å¤æ•°æ®é›†å…ƒæ•°æ®çš„è„šæœ¬

æ£€æŸ¥å¹¶ä¿®å¤ dataset_to_index ä¸å®é™…æ•°æ®ä¸åŒ¹é…çš„é—®é¢˜
"""

from lerobot.datasets.lerobot_dataset import LeRobotDataset
import json
from pathlib import Path
import pandas as pd


def fix_dataset_metadata(repo_id: str, root: str | None = None):
    """ä¿®å¤æ•°æ®é›†å…ƒæ•°æ®"""
    
    # åŠ è½½æ•°æ®é›†
    dataset = LeRobotDataset(repo_id, root=root)
    
    print(f"ğŸ“Š æ•°æ®é›†ä¿¡æ¯:")
    print(f"  Repo ID: {repo_id}")
    print(f"  æ€»å¸§æ•°: {len(dataset)}")
    print(f"  Episode æ•°: {len(dataset.meta.episodes)}")
    
    # æ£€æŸ¥æ¯ä¸ª episode çš„ç´¢å¼•
    print(f"\nğŸ” æ£€æŸ¥ Episode ç´¢å¼•:")
    
    # â­ ä¿®å¤ï¼šåˆ¤æ–­ episodes çš„ç±»å‹å¹¶è½¬æ¢ä¸º DataFrame
    episodes = dataset.meta.episodes
    
    # æ£€æŸ¥ç±»å‹å¹¶æ‰“å°è°ƒè¯•ä¿¡æ¯
    print(f"  Episodes ç±»å‹: {type(episodes)}")
    
    if isinstance(episodes, pd.DataFrame):
        # å¦‚æœå·²ç»æ˜¯ DataFrameï¼Œç›´æ¥å¤åˆ¶
        episodes_df = episodes.copy()
    else:
        # å¦‚æœæ˜¯ HuggingFace Datasetï¼Œè½¬æ¢ä¸º DataFrame
        try:
            # æ–¹æ³• 1ï¼šä½¿ç”¨ to_pandas()
            episodes_df = episodes.to_pandas()
        except AttributeError:
            try:
                # æ–¹æ³• 2ï¼šæ‰‹åŠ¨æ„å»º DataFrame
                episodes_dict = {
                    'episode_index': episodes['episode_index'],
                    'dataset_from_index': episodes['dataset_from_index'],
                    'dataset_to_index': episodes['dataset_to_index'],
                    'length': episodes['length'],
                }
                episodes_df = pd.DataFrame(episodes_dict)
            except Exception as e:
                print(f"âŒ æ— æ³•è½¬æ¢ episodes ä¸º DataFrame: {e}")
                print(f"   Episodes ç»“æ„: {episodes}")
                return
    
    print(f"  è½¬æ¢åçš„ DataFrame å½¢çŠ¶: {episodes_df.shape}")
    print(f"  åˆ—å: {episodes_df.columns.tolist()}")
    
    has_error = False
    
    for idx, ep in episodes_df.iterrows():
        ep_idx = ep['episode_index']
        from_idx = ep['dataset_from_index']
        to_idx = ep['dataset_to_index']
        length = ep['length']
        
        # æ£€æŸ¥ to_idx æ˜¯å¦è¶…å‡ºèŒƒå›´
        if to_idx > len(dataset):
            print(f"  âŒ Episode {ep_idx}: to_idx={to_idx} > æ€»å¸§æ•°={len(dataset)}")
            has_error = True
            
            # ä¿®å¤ï¼šå°† to_idx è®¾ä¸ºå®é™…çš„æœ€å¤§ç´¢å¼•
            corrected_to_idx = len(dataset)
            corrected_length = corrected_to_idx - from_idx
            
            print(f"     ä¿®å¤å»ºè®®: to_idx={corrected_to_idx}, length={corrected_length}")
            
            # æ›´æ–° DataFrame
            episodes_df.at[idx, 'dataset_to_index'] = corrected_to_idx
            episodes_df.at[idx, 'length'] = corrected_length
        else:
            print(f"  âœ… Episode {ep_idx}: [{from_idx}, {to_idx}), length={length}")
    
    if has_error:
        print(f"\nâš ï¸  å‘ç°å…ƒæ•°æ®é”™è¯¯ï¼Œæ­£åœ¨ä¿®å¤...")
        
        # æ‰¾åˆ°æ•°æ®é›†çš„æœ¬åœ°è·¯å¾„
        from huggingface_hub import snapshot_download
        local_dir = Path(snapshot_download(repo_id, repo_type='dataset'))
        
        print(f"\nğŸ“‚ æ•°æ®é›†æœ¬åœ°è·¯å¾„: {local_dir}")
        
        # æŸ¥æ‰¾ episodes.jsonl æˆ– meta.json
        possible_files = [
            local_dir / "meta" / "episodes.jsonl",
            local_dir / "episodes.jsonl",
            local_dir / "meta" / "info.json",
            local_dir / "meta.json",
            local_dir / "info.json",
        ]
        
        meta_file = None
        for file_path in possible_files:
            if file_path.exists():
                meta_file = file_path
                print(f"  âœ… æ‰¾åˆ°å…ƒæ•°æ®æ–‡ä»¶: {file_path}")
                break
        
        if meta_file is None:
            print(f"  âŒ æœªæ‰¾åˆ°å…ƒæ•°æ®æ–‡ä»¶ï¼Œå°è¯•çš„è·¯å¾„:")
            for p in possible_files:
                print(f"     - {p}")
            print(f"\nğŸ’¡ å»ºè®®ï¼šæ‰‹åŠ¨æŸ¥æ‰¾æˆ–é‡æ–°ç”Ÿæˆæ•°æ®é›†")
            return
        
        # å¤‡ä»½åŸå§‹æ–‡ä»¶
        backup_path = meta_file.with_suffix(meta_file.suffix + '.backup')
        import shutil
        shutil.copy(meta_file, backup_path)
        print(f"  ğŸ“¦ å·²å¤‡ä»½åŸå§‹æ–‡ä»¶åˆ°: {backup_path}")
        
        # æ ¹æ®æ–‡ä»¶ç±»å‹è¿›è¡Œå¤„ç†
        if meta_file.suffix == '.jsonl':
            # å¤„ç† JSONL æ ¼å¼ï¼ˆepisodes.jsonlï¼‰
            print(f"\n  å¤„ç† JSONL æ ¼å¼...")
            
            # å†™å…¥ä¿®å¤åçš„ episodes
            with open(meta_file, 'w') as f:
                for _, row in episodes_df.iterrows():
                    episode_dict = row.to_dict()
                    f.write(json.dumps(episode_dict) + '\n')
            
            print(f"  âœ… å·²ä¿å­˜ä¿®å¤åçš„ episodes.jsonl")
            
        elif meta_file.suffix == '.json':
            # å¤„ç† JSON æ ¼å¼ï¼ˆmeta.json æˆ– info.jsonï¼‰
            print(f"\n  å¤„ç† JSON æ ¼å¼...")
            
            # è¯»å–åŸå§‹æ–‡ä»¶
            with open(meta_file, 'r') as f:
                meta_dict = json.load(f)
            
            # æ›´æ–° episodes ä¿¡æ¯
            meta_dict['episodes'] = episodes_df.to_dict('records')
            
            # å†™å›æ–‡ä»¶
            with open(meta_file, 'w') as f:
                json.dump(meta_dict, f, indent=2)
            
            print(f"  âœ… å·²ä¿å­˜ä¿®å¤åçš„ {meta_file.name}")
        
        print(f"\nğŸ‰ ä¿®å¤å®Œæˆï¼")
        print(f"\nâš ï¸  é‡è¦ï¼šéœ€è¦æ¸…é™¤ HuggingFace ç¼“å­˜æ‰èƒ½ç”Ÿæ•ˆï¼")
        print(f"\nè¿è¡Œä»¥ä¸‹å‘½ä»¤æ¸…é™¤ç¼“å­˜:")
        print(f"  rm -rf ~/.cache/huggingface/datasets/{repo_id.replace('/', '___')}")
        print(f"\nç„¶åé‡æ–°åŠ è½½æ•°æ®é›†å³å¯ã€‚")
        
        # æ‰“å°ä¿®å¤åçš„ episodes ä¿¡æ¯
        print(f"\nğŸ“Š ä¿®å¤åçš„ Episode ä¿¡æ¯:")
        print(episodes_df)
        
    else:
        print(f"\nâœ… å…ƒæ•°æ®æ£€æŸ¥é€šè¿‡ï¼Œæ— éœ€ä¿®å¤ã€‚")
        print(f"\nğŸ“Š Episode ä¿¡æ¯:")
        print(episodes_df)


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="ä¿®å¤æ•°æ®é›†å…ƒæ•°æ®")
    parser.add_argument("--repo-id", type=str, required=True, help="æ•°æ®é›† repo ID")
    parser.add_argument("--root", type=str, default=None, help="æ•°æ®é›†æ ¹ç›®å½•")
    
    args = parser.parse_args()
    
    fix_dataset_metadata(args.repo_id, args.root)